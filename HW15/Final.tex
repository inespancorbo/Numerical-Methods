\PassOptionsToPackage{unicode=true}{hyperref} % options for packages loaded elsewhere
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provides euro and other symbols
\else % if luatex or xelatex
  \usepackage{unicode-math}
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage[]{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\usepackage{hyperref}
\hypersetup{
            pdftitle={HW 15},
            pdfauthor={Ines Pancorbo},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

% set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother


\title{HW 15}
\author{Ines Pancorbo}
\date{}

\begin{document}
\maketitle

\hypertarget{section}{%
\section{1.}\label{section}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"ggplot2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sequences <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"matrix_sequences.csv"}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ F, }
                           \DataTypeTok{as.is =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{colClasses =} \StringTok{"factor"}\NormalTok{))}
\KeywordTok{colnames}\NormalTok{(data) <-}\StringTok{ }\OtherTok{NULL}
\KeywordTok{rownames}\NormalTok{(data) <-}\StringTok{ }\OtherTok{NULL}

\NormalTok{reference <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"matrix_reference.csv"}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ F, }
                                \DataTypeTok{as.is =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{colClasses =} \StringTok{"factor"}\NormalTok{))}
\KeywordTok{colnames}\NormalTok{(reference) <-}\StringTok{ }\OtherTok{NULL}
\KeywordTok{rownames}\NormalTok{(reference) <-}\StringTok{ }\OtherTok{NULL}

\NormalTok{countries <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(}\KeywordTok{read.csv}\NormalTok{(}\StringTok{"matrix_countries.csv"}\NormalTok{, }\DataTypeTok{header =}\NormalTok{ F, }
                                \DataTypeTok{as.is =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{colClasses =} \StringTok{"factor"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(sequences)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   241 29903
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(reference)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]     1 29903
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{dim}\NormalTok{(countries)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 241   1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# If a sequence has the same letter at a given position as }
\CommentTok{# the reference, set the matrix entry to 1, otherwise set it to 0 }
\NormalTok{X <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(sequences, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{as.numeric}\NormalTok{(x }\OperatorTok{==}\StringTok{ }\NormalTok{reference)))}
\KeywordTok{dim}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1]   241 29903
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let us center the sequences}
\NormalTok{X <-}\StringTok{ }\KeywordTok{t}\NormalTok{(}\KeywordTok{apply}\NormalTok{(X, }\DecValTok{1}\NormalTok{, }\ControlFlowTok{function}\NormalTok{(x) x}\OperatorTok{-}\KeywordTok{colMeans}\NormalTok{(X)))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{norm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{))}

\CommentTok{# power iteration approach}
\NormalTok{power_iteration <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(X, start) \{}
\NormalTok{  start1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(start[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{nrow =} \KeywordTok{ncol}\NormalTok{(X), }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{)}\OperatorTok{/}\KeywordTok{norm}\NormalTok{(start[,}\DecValTok{1}\NormalTok{])}
\NormalTok{  start2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(start[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{nrow =} \KeywordTok{ncol}\NormalTok{(X), }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{)}\OperatorTok{/}\KeywordTok{norm}\NormalTok{(start[,}\DecValTok{2}\NormalTok{])}
  \CommentTok{# not computing X^TX}
\NormalTok{  RQ1 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(start1) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{start1))}
\NormalTok{  RQ2 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(start2) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{start2))}
  
  \ControlFlowTok{repeat}\NormalTok{ \{}
    
\NormalTok{    start <-}\StringTok{ }\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{start)}
\NormalTok{    start <-}\StringTok{ }\KeywordTok{qr.Q}\NormalTok{(}\KeywordTok{qr}\NormalTok{(start))}
    
\NormalTok{    start1 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(start[,}\DecValTok{1}\NormalTok{],}\DataTypeTok{nrow =} \KeywordTok{ncol}\NormalTok{(X), }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{)}\OperatorTok{/}\KeywordTok{norm}\NormalTok{(start[,}\DecValTok{1}\NormalTok{])}
\NormalTok{    start2 <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(start[,}\DecValTok{2}\NormalTok{],}\DataTypeTok{nrow =} \KeywordTok{ncol}\NormalTok{(X), }\DataTypeTok{ncol =} \DecValTok{1}\NormalTok{)}\OperatorTok{/}\KeywordTok{norm}\NormalTok{(start[,}\DecValTok{2}\NormalTok{])}
    
\NormalTok{    new_RQ1 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(start1) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{start1))}
\NormalTok{    new_RQ2 <-}\StringTok{ }\KeywordTok{t}\NormalTok{(start2) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(}\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{start2))}
    
    \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{abs}\NormalTok{(new_RQ1 }\OperatorTok{-}\StringTok{ }\NormalTok{RQ1) }\OperatorTok{<}\StringTok{ }\DecValTok{10}\OperatorTok{^-}\DecValTok{10} \OperatorTok{&&}\StringTok{ }\KeywordTok{abs}\NormalTok{(new_RQ2 }\OperatorTok{-}\StringTok{ }\NormalTok{RQ2) }\OperatorTok{<}\StringTok{ }\DecValTok{10}\OperatorTok{^-}\DecValTok{10}\NormalTok{)\{}
      \ControlFlowTok{break} 
\NormalTok{    \}}
    \ControlFlowTok{else}\NormalTok{\{}
\NormalTok{      RQ1 <-}\StringTok{ }\NormalTok{new_RQ1}
\NormalTok{      RQ2 <-}\StringTok{ }\NormalTok{new_RQ2}
\NormalTok{    \}}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{ (}\KeywordTok{list}\NormalTok{(}\DataTypeTok{principal_components =}\NormalTok{ start, }\DataTypeTok{lambda1 =}\NormalTok{ RQ1, }\DataTypeTok{lambda2 =}\NormalTok{ RQ2)) }
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# let us get the first two principal components of X, i.e.,}
\CommentTok{# two most dominant eigenvectors of X^TX}
\NormalTok{start <-}\StringTok{ }\KeywordTok{matrix}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\KeywordTok{ncol}\NormalTok{(X)}\OperatorTok{*}\DecValTok{2}\NormalTok{), }\DataTypeTok{nrow=}\KeywordTok{ncol}\NormalTok{(X), }\DataTypeTok{ncol=}\DecValTok{2}\NormalTok{)}
\NormalTok{start_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{power_iteration_result <-}\StringTok{ }\KeywordTok{power_iteration}\NormalTok{(X, start)}
\NormalTok{end_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{() }
\NormalTok{end_time  }\OperatorTok{-}\StringTok{ }\NormalTok{start_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time difference of 11.23493 secs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pcs <-}\StringTok{ }\NormalTok{power_iteration_result}\OperatorTok{$}\NormalTok{principal_components}
\end{Highlighting}
\end{Shaded}

Let us check we got the correct first two principal components of \(X\)
by computing the SVD of \(X\). The column vectors of the matrix \(V\)
are the eigenvectors of \(X^TX\) (from most dominant to least), and
consequently the first two column vectors of \(V\) are the first two
principal components of \(X\). Also note that computing the SVD takes
less than doing power iteration. Let us check that the eigenvalues
calculated via power iteration are the same as the ones given by R's
svd().

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{start_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{svd.X <-}\StringTok{ }\KeywordTok{svd}\NormalTok{(X)}
\NormalTok{end_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{() }
\NormalTok{end_time  }\OperatorTok{-}\StringTok{ }\NormalTok{start_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time difference of 5.804312 secs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the two most dominant eigenvalues of X^TX are (using power iteration)}
\NormalTok{power_iteration_result}\OperatorTok{$}\NormalTok{lambda1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 319.9156
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{power_iteration_result}\OperatorTok{$}\NormalTok{lambda2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##          [,1]
## [1,] 120.3299
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the two most dominant eigenvalues of X^TX are (using R's svd() function)}
\NormalTok{svd.X}\OperatorTok{$}\NormalTok{d[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 319.9156
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{svd.X}\OperatorTok{$}\NormalTok{d[}\DecValTok{2}\NormalTok{]}\OperatorTok{^}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 120.3299
\end{verbatim}

Let's determine the fraction of the variance captured by the 2-d PCA. We
could do this in a couple different ways. Here are two:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let the 2-d projection of X be X2.}
\CommentTok{# We could compute the frobenius norm of X-X2 and X}
\CommentTok{# Let's get X2 first}
\NormalTok{start_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\NormalTok{X2 <-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\NormalTok{pcs }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(pcs)}
\NormalTok{fnorm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sum}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\CommentTok{# variance captured by the 2-d PCA is}
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{fnorm}\NormalTok{(X}\OperatorTok{-}\NormalTok{X2)}\OperatorTok{/}\KeywordTok{fnorm}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3214464
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the first pc captures}
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{fnorm}\NormalTok{(X}\OperatorTok{-}\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{pcs[,}\DecValTok{1}\NormalTok{] }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(pcs[,}\DecValTok{1}\NormalTok{])))}\OperatorTok{/}\KeywordTok{fnorm}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2335872
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the second pc captures}
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{fnorm}\NormalTok{(X}\OperatorTok{-}\NormalTok{(X }\OperatorTok{%*%}\StringTok{ }\NormalTok{pcs[,}\DecValTok{2}\NormalTok{] }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(pcs[,}\DecValTok{2}\NormalTok{])))}\OperatorTok{/}\KeywordTok{fnorm}\NormalTok{(X)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08785917
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{end_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{() }
\NormalTok{end_time  }\OperatorTok{-}\StringTok{ }\NormalTok{start_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time difference of 0.5045831 secs
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# We could use R's svd() function and look at the singular values}
\CommentTok{# We want the eigenvalues of X^TX, which are the squares of the singular values of X }
\CommentTok{# And so the variance captured by the 2-d PCA is}
\NormalTok{start_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{()}
\KeywordTok{sum}\NormalTok{(svd.X}\OperatorTok{$}\NormalTok{d[}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(svd.X}\OperatorTok{$}\NormalTok{d}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3214464
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the first pc captures}
\NormalTok{svd.X}\OperatorTok{$}\NormalTok{d[}\DecValTok{1}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(svd.X}\OperatorTok{$}\NormalTok{d}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2335872
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# the second pc captures}
\NormalTok{svd.X}\OperatorTok{$}\NormalTok{d[}\DecValTok{2}\NormalTok{]}\OperatorTok{^}\DecValTok{2}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(svd.X}\OperatorTok{$}\NormalTok{d}\OperatorTok{^}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.08785917
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{end_time <-}\StringTok{ }\KeywordTok{Sys.time}\NormalTok{() }
\NormalTok{end_time  }\OperatorTok{-}\StringTok{ }\NormalTok{start_time}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Time difference of 0.00425601 secs
\end{verbatim}

The 2-d PCA captures approx \(32.14\)\% of the variance (approx
\(23.36\)\% by pc1 and approx \(8.79\)\% by pc2). It takes less time
using R's svd() function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Before projecting let's look at how many genome samples}
\CommentTok{# per country}
\NormalTok{freq_table <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\KeywordTok{table}\NormalTok{(countries))}
\KeywordTok{ggplot}\NormalTok{(freq_table, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =} \KeywordTok{reorder}\NormalTok{(countries, }\OperatorTok{-}\NormalTok{Freq), }\DataTypeTok{y =}\NormalTok{ Freq)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_col}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"#00AFBB"}\NormalTok{)}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom_text}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{label =}\NormalTok{ Freq)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{""}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme_light}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{axis.text.x =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{8}\NormalTok{, }\DataTypeTok{angle =} \DecValTok{90}\NormalTok{),}
        \DataTypeTok{legend.position =} \StringTok{"none"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Final_files/figure-latex/unnamed-chunk-12-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# projecting onto the two most dominant principal components}
\CommentTok{# getting coordinates}
\NormalTok{c <-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\NormalTok{pcs}

\NormalTok{data <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ c[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ c[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{country =} \KeywordTok{c}\NormalTok{(countries))}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# let's create a color palette because R's default is too light}
\CommentTok{# make the countries with > 1 genome data point more visible}
\NormalTok{countries_sorted <-}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(freq_table[}\KeywordTok{order}\NormalTok{(freq_table[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{decreasing =}\NormalTok{ T),][,}\DecValTok{1}\NormalTok{])}
\NormalTok{colors <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"#800000"}\NormalTok{, }\StringTok{"#808000"}\NormalTok{, }\StringTok{"#008000"}\NormalTok{, }\StringTok{"#000080"}\NormalTok{, }\StringTok{"#FF6347"}\NormalTok{, }
            \StringTok{"#FFD700"}\NormalTok{, }\StringTok{"#00CED1"}\NormalTok{, }\StringTok{"#8A2BE2"}\NormalTok{, }\StringTok{"#BC8F8F"}\NormalTok{, }\StringTok{"#FF00FF"}\NormalTok{, }\StringTok{"#00FF00"}\NormalTok{,}
            \StringTok{"#000000"}\NormalTok{, }\StringTok{"#C0C0C0"}\NormalTok{, }\StringTok{"#2F4F4F"}\NormalTok{, }\StringTok{"#00FF00"}\NormalTok{, }\StringTok{"#FFFACD"}\NormalTok{, }\StringTok{"#FF0000"}\NormalTok{,}
            \StringTok{"#0000FF"}\NormalTok{, }\StringTok{"#800080"}\NormalTok{, }\StringTok{"#7FFFD4"}\NormalTok{, }\StringTok{"#2F4F4F"}\NormalTok{, }\StringTok{"#FFA07A"}\NormalTok{)}

\KeywordTok{names}\NormalTok{(colors) <-}\StringTok{ }\NormalTok{countries_sorted}

\KeywordTok{ggplot}\NormalTok{(data) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{color =}\NormalTok{ country), }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_manual}\NormalTok{(}\StringTok{"countries"}\NormalTok{, }\DataTypeTok{values =}\NormalTok{ colors) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Principal Component 1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Principal Component 2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Final_files/figure-latex/unnamed-chunk-14-1.pdf}

Let's check that we get the same plot using R's svd().

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# gettting the two most dominant principal components}
\NormalTok{pcs_svd <-}\StringTok{ }\NormalTok{svd.X}\OperatorTok{$}\NormalTok{v[,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}

\CommentTok{# projecting onto the two most dominant principal components}
\CommentTok{# getting coordinates}
\NormalTok{c_svd <-}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\NormalTok{pcs_svd}

\NormalTok{data_svd <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ c_svd[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ c_svd[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{country =} \KeywordTok{c}\NormalTok{(countries))}

\KeywordTok{ggplot}\NormalTok{(data_svd) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{color =}\NormalTok{ country), }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_manual}\NormalTok{(}\StringTok{"countries"}\NormalTok{, }\DataTypeTok{values =}\NormalTok{ colors) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Principal Component 1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Principal Component 2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Final_files/figure-latex/unnamed-chunk-15-1.pdf}

We get the same plot (reflected, but eigenvectors are not unique, they
are unique up to a scalar).

\textbf{Comment on the implications for how the virus spread.}

Some observations from the plot: It looks like there are four clusters.
Patients from China are present in two of them. Patients from the US
dominate two of them as well but are more spead out. As seen from the
barplot, we have many data points corresponding to patients from Taiwan
and Spain too, and in both cases they are, for the most part, spread
out.

Let's plot according to geographic region.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data_europe <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, country }\OperatorTok{%in%}\StringTok{ }
\StringTok{                        }\KeywordTok{c}\NormalTok{(}\StringTok{"Finland"}\NormalTok{, }\StringTok{"France"}\NormalTok{, }\StringTok{"Greece"}\NormalTok{, }\StringTok{"Italy"}\NormalTok{, }\StringTok{"Spain"}\NormalTok{,}
                          \StringTok{"Sweden"}\NormalTok{, }\StringTok{"Turkey"}\NormalTok{))}
\NormalTok{data_asia <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, country }\OperatorTok{%in%}\StringTok{ }
\StringTok{                      }\KeywordTok{c}\NormalTok{(}\StringTok{"China"}\NormalTok{, }\StringTok{"India"}\NormalTok{, }\StringTok{"Iran"}\NormalTok{, }\StringTok{"Israel"}\NormalTok{, }\StringTok{"Japan"}\NormalTok{,}
                        \StringTok{"Malaysia"}\NormalTok{, }\StringTok{"Nepal"}\NormalTok{, }\StringTok{"Pakistan"}\NormalTok{, }\StringTok{"Taiwan"}\NormalTok{, }\StringTok{"Australia"}\NormalTok{))}
\NormalTok{data_america <-}\StringTok{ }\KeywordTok{subset}\NormalTok{(data, country }\OperatorTok{%in%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Brazil"}\NormalTok{, }\StringTok{"Colombia"}\NormalTok{, }\StringTok{"USA"}\NormalTok{, }\StringTok{"Peru"}\NormalTok{))}

\KeywordTok{ggplot}\NormalTok{(data_europe) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{color =}\NormalTok{ country), }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_manual}\NormalTok{(}\StringTok{"Europe"}\NormalTok{, }\DataTypeTok{values =}\NormalTok{ colors) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Principal Component 1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Principal Component 2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Final_files/figure-latex/unnamed-chunk-16-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(data_asia) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{color =}\NormalTok{ country), }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_manual}\NormalTok{(}\StringTok{"Asia"}\NormalTok{, }\DataTypeTok{values =}\NormalTok{ colors) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Principal Component 1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Principal Component 2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Final_files/figure-latex/unnamed-chunk-16-2.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(data_america) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ x, }\DataTypeTok{y =}\NormalTok{ y, }\DataTypeTok{color =}\NormalTok{ country), }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{6}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_colour_manual}\NormalTok{(}\StringTok{"America"}\NormalTok{, }\DataTypeTok{values =}\NormalTok{ colors) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"Principal Component 1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Principal Component 2"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Final_files/figure-latex/unnamed-chunk-16-3.pdf}

From the plots above: For the most part, for those countries with more
than \(1\) datapoint, we can see that the corresponding sequences are
mapped closely. This is true for Brazil, Malaysia, Japan, Italy.
Sequences belonging to patients from countries present in various
clusters (US, Taiwan, China, US) are generally mapped closely within the
cluster (US and China are clear examples).

It doesn't look like sequences corresponding to patients from countries
close geographically are mapped closely. For example, sequences from
patients in Brazil are mapped closely to sequences from patients in
Spain or Turkey and Taiwan or Malaysia. This is probably because of
tourism and being able to fly in/out of countries.

\hypertarget{section-1}{%
\section{2.}\label{section-1}}

\hypertarget{a}{%
\section{2. (a)}\label{a}}

Since we want \(w \backepsilon w^T\phi(x)\) is conformable, it must be
that \(w \in R^m.\) So dimension is \(m\).

We have data \((x^{(i)},y_i)\) where
\(x^{(i)} \in R^n, i=1,2, \cdots, N.\)

We define features through the mapping \(\phi(x): R^n \to R^m\) and so
we have data \((\phi(x^{(i)}),y_i)\).

We are given that
\(P(y_i=1|w,\phi(x^{(i)}))=\frac{1}{1+e^{-w^T\phi(x^{(i)})}}.\)

We can assume independence and we can think of each of the response
\(y_i\) as conditionally Bernoulli distributed. So we have
\(Y_i|\phi(x^{(i)}) \sim \text{Bernoulli}(p)\) where
\(p=P(Y_i=1|\phi(x^{(i)});w)=\frac{1}{1+e^{-w^T\phi(x^{(i)})}}\)

This implies \[f_{Y_i|\phi(x^{(i)})}(y_i)=p^{y_i}(1-p)^{1-y_i}\]
\[=P(Y_i=1|\phi(x^{(i)});w)^{y_i}(1-P(Y_i=1|\phi(x^{(i)});w)^{1-y_i}\]
\[=(\frac{1}{1+e^{-w^T\phi(x^{(i)})}})^{y_i}(1-\frac{1}{1+e^{-w^T\phi(x^{(i)})}})^{1-y_i}~~~\text{for}~~~y_i \in \{0,1\}~~~\text{and}~~~0~~~\text{otherwise}.\]

Consequently, the likelihood function is
\[L(w)= \displaystyle \prod_{i=1}^{N}p^{y_i}(1-p)^{1-y_i}\]
\[=\displaystyle \prod_{i=1}^{N}(\frac{1}{1+e^{-w^T\phi(x^{(i)})}})^{y_i}(1-\frac{1}{1+e^{-w^T\phi(x^{(i)})}})^{1-y_i}.\]

Thus, the log likelihood function is

\[\text{log} L(w)=\displaystyle\sum_{i=1}^{N}(y_i\text{log}(\frac{1}{1+e^{-w^T\phi(x^{(i)})}})+(1-y_i)\text{log}(1-\frac{1}{1+e^{-w^T\phi(x^{(i)})}}))\]
\[=\displaystyle \sum_{i=1}^{N}(y_i\text{log}(1)-y_i\text{log}(1+e^{-w^T\phi(x^{(i)})})+(1-y_i)\text{log}(\frac{e^{-w^T\phi(x^{(i)})}}{1+e^{-w^T\phi(x^{(i)})}}))\]
\[= \displaystyle \sum_{i=1}^{N}(-y_i\text{log}(1+e^{-w^T\phi(x^{(i)})})+(1-y_i)(-w^T\phi(x^{(i)}))-(1-y_i)\text{log}(1+e^{-w^T\phi(x^{(i)})})\]
\[= \displaystyle \sum_{i=1}^{N}((1-y_i)(-w^T\phi(x^{(i)}))+(-1+y_i-y_i)\text{log}(1+e^{-w^T\phi(x^{(i)})}))\]
\[=\displaystyle \sum_{i=1}^{N}((1-y_i)(-w^T\phi(x^{(i)}))-\text{log}(1+e^{-w^T\phi(x^{(i)})})).\]

\hypertarget{b-i.}{%
\section{2. (b) i.}\label{b-i.}}

Consider
\(E=\text{span}(\phi(x^{(1)}), \phi(x^{(2)}), \cdots, \phi(x^{(N)}))\).
Then \(E \subset R^m.\) And any \(v \in R^m\) can be represented as
\(u+z\), where \(u\in E\) and \(z\in E^{\perp}.\) \(u\) will be the
orthogonal projection of \(v\) onto \(E\) and \(z=v-u\) will be the
component of \(v\) orthogonal to \(E\).

Let \(B\) be the model matrix for the features of the sampled data.
Considering the above, we can write the \(w\) in the log likelihood,
which is in \(R^m\), as \(B^Ta + z\), where \(B^Ta\) is the orthogonal
projection of \(w\) onto \(E\) and \(z\) is the component of \(w\)
orthogonal to \(E\).

Let us write \(w = B^Ta + z\). We want to show that we can write the
\(w\) that maximizes the log likelihod, \(w^*\), as \(w^*=B^Ta^*\) for
some \(a^* \in R^N\). In other words, that \(w^* \in E\).

Consider the maximization problem:

\[\max_{w \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-w^T\phi(x^{(i)}))-\text{log}(1+e^{-w^T\phi(x^{(i)})}))\]
\[=\max_{w \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^Tw)-\text{log}(1+e^{-\phi(x^{(i)})^Tw}))\]

Let us substitute \(w\) for \(B^Ta + z\) and consider the maximization
problem again. Note \(a \in R^N, z \in R^m.\)

\[\max_{a \in R^N,z \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^T(B^Ta + z))-\text{log}(1+e^{-\phi(x^{(i)})^T(B^Ta + z)}))\]
\[=\max_{a \in R^N,z \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^TB^Ta -\phi(x^{(i)})^T z))-\text{log}(1+e^{-\phi(x^{(i)})^TB^Ta -\phi(x^{(i)})^Tz)}))\]
Now, since \(z\) is orthogonal to each of the \(\phi(x^{(i)})\) we get
the following simplification:

\[=\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^TB^Ta)-\text{log}(1+e^{-\phi(x^{(i)})^TB^Ta}))\]
\[=\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-(B\phi(x^{(i)}))^Ta)-\text{log}(1+e^{-(B\phi(x^{(i)}))^Ta}))\]
\[=\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^TB\phi(x^{(i)}))-\text{log}(1+e^{-a^TB\phi(x^{(i)})}))\]
So the above shows that in terms of the maximization problem, \(z\)
plays no role. So \(w^*=B^Ta^*\) for some \(a^* \in R^N\). Thus,
\(w^* \in E=\text{span}(\phi(x^{(1)}), \phi(x^{(2)}), \cdots, \phi(x^{(N)})).\)

Aside: More compactly you could write the summation above as,
\[-(1-y)^TBB^Ta-\text{log}(1+e^{-BB^Ta})\]

\hypertarget{b-ii.}{%
\section{2. (b) ii.}\label{b-ii.}}

From above, we have the maximization problem reduced to:

\[\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^TB\phi(x^{(i)}))-\text{log}(1+e^{-a^TB\phi(x^{(i)})}))\]
Now consider the \(N \times N\) matrix \(BB^T\),

\[ 
B B^T = 
\begin{pmatrix}
\phi(x^{(1)})^T \\
\phi(x^{(2)})^T  \\
\phi(x^{(3)})^T \\
\vdots    \\
\phi(x^{(N)})^T
\end{pmatrix}
\begin{pmatrix}
\phi(x^{(1)}) & \phi(x^{(2)}) & \cdots & \phi(x^{(N)})
\end{pmatrix} 
\] \[
=\begin{pmatrix}
\phi(x^{(1)})^T\phi(x^{(1)}) & \phi(x^{(1)})^T\phi(x^{(2)}) & \cdots & \phi(x^{(1)})^T\phi(x^{(N)}) \\
\vdots  & \vdots & \ddots & \vdots  \\
\phi(x^{(N)})^T\phi(x^{(1)}) & \phi(x^{(N)})^T\phi(x^{(2)}) & \cdots & \phi(x^{(N)})^T\phi(x^{(N)}) \\
\end{pmatrix}
\] Then from above, \(BB^T\) is a \(N \times N\) kernel matrix since
each entry \(BB^T_{jl}=\phi(x^{(j)})^T\phi(x^{(l)})\). Denote \(BB^T\)
by \(K\). Each entry \(K_{jl}\) of \(K\) is then
\(\phi(x^{(j)})^T\phi(x^{(l)}).\)

Now, the ith column, \(k^{(i)}\), of \(K\) is the ith column,
\((BB^T)_i\), of \(BB^T\),

\[
k^{(i)}=(BB^T)_i=
\begin{pmatrix}
\phi(x^{(1)})^T\phi(x^{(i)}) \\
\phi(x^{(2)})^T\phi(x^{(i)})  \\
\phi(x^{(3)})^T\phi(x^{(i)}) \\
\vdots    \\
\phi(x^{(N)})^T\phi(x^{(i)})
\end{pmatrix}
= 
\begin{pmatrix}
\phi(x^{(1)})^T \\
\phi(x^{(2)})^T  \\
\phi(x^{(3)})^T \\
\vdots    \\
\phi(x^{(N)})^T
\end{pmatrix}
\phi(x^{(i)})=B\phi(x^{(i)})
\] And so substituting \(B\phi(x^{(i)})\) with \(k^{(i)}\) we have,

\[\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^TB\phi(x^{(i)}))-\text{log}(1+e^{-a^TB\phi(x^{(i)})}))\]

\[=\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^Tk^{(i)})-\text{log}(1+e^{-a^Tk^{(i)}}))\]

Aside: More compactly you could write the summation above as,
\[-(1-y)^TKa-\text{log}(1+e^{-Ka})\]

\hypertarget{b-iii.}{%
\section{2. (b) iii.}\label{b-iii.}}

Given \(x \in R^n,\) we know
\[P(y=1|w^*,\phi(x))=\frac{1}{1+e^{-(w^*)^{T}\phi(x)}}.\] In ii. we
showed that \(w^*=B^Ta^*\) and so,
\[P(y=1|a^*,\phi(x))=\frac{1}{1+e^{-(B^Ta^*)^{T}\phi(x)}}\]
\[=\frac{1}{1+e^{-(a^*)^{T}B\phi(x)}}\] Now,

\[B\phi(x)=
\begin{pmatrix}
\phi(x^{(1)})^T \\
\phi(x^{(2)})^T  \\
\phi(x^{(3)})^T \\
\vdots    \\
\phi(x^{(N)})^T
\end{pmatrix}\phi(x)
=
\begin{pmatrix}
\phi(x^{(1)})^T\phi(x) \\
\phi(x^{(2)})^T\phi(x)  \\
\phi(x^{(3)})^T\phi(x) \\
\vdots    \\
\phi(x^{(N)})^T\phi(x)
\end{pmatrix}
\] And so by denoting \(B\phi(x) \in R^N\) as \(\tilde{k}\) (from above
one can see that the ith entry, \(\tilde{k}_i\), of \(\tilde{k}\) is
\(\phi(x^{(i)})^T\phi(x))\) we have:

\[P(y=1|a^*,\phi(x))=\frac{1}{1+e^{-(a^*)^{T}\tilde{k}}}\]

\hypertarget{c}{%
\section{2. (c)}\label{c}}

\((3)\) and \((4)\) would be advantageous when \(m >> N\) and \(N\) is
not unreasonably ``large'' (otherwise, we could resort to Neural Nets).

Note, we saw that we would have to solve the following maximization
problem:
\[\max_{w \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^Tw)-\text{log}(1+e^{-\phi(x^{(i)})^Tw}))\]
which is equivalent to maximizing for \(w\)
\[-(1-y)^TBw-\text{log}(1+e^{-B w})\] The above requires being able to
store \(B\). This implies being able to store a \(N \times m\) matrix.
If \(m >> N\), we might not be able to store this matrix or even
compute, in a reasonable amount of time, \(Bw\) which requires \(Nm\)
multiplications.

So in this case, resorting to \((3)\) (and by extension, for prediction,
\((4)\)), i.e.,

\[\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^Tk^{(i)})-\text{log}(1+e^{-a^Tk^{(i)}}))\]
which is equivalent to maximizing for a
\[-(1-y)^TKa-\text{log}(1+e^{-Ka})\]

is advantageous since it requieres storing \(K\), which is
\(N \times N\), and computing \(Ka\) which requires \(N^2\)
multiplications, which is less than \(Nm\) multiplications.

\hypertarget{d-i.}{%
\section{2. (d) i.}\label{d-i.}}

Consider,
\[g_i(w)=(1-y_i)(w^T\phi(x^{(i)}))+\text{log}(1+e^{-w^T\phi(x^{(i)}}))\]

From HW \(7\), we know the Hessian of \(g_i(w)\) is,

\[\text{H g}_i(w)=\phi(x^{(i)})\phi(x^{(i)})^T\big[\frac{e^{-w^T\phi(x^{(i)})}}{(1+e^{-w^T\phi(x^{(i)})})^2}\big]\]

The above is using what we showed in HW \(7\), \(3\).b.ii), substituting
\(\alpha\) with \(w\) and the vector \(\tilde{x_i}\) with the vector
\(\phi(x^{(i)})\).

Then given any \(x \in R^m,\)

\[x^T\text{H g}_i(w)x\]
\[=(\frac{e^{-w^T\phi(x^{(i)})}}{(1+e^{-w^T\phi(x^{(i)})})^2})x^T(\phi(x^{(i)})\phi(x^{(i)})^T)x\]
\[=(\frac{e^{-w^T\phi(x^{(i)})}}{(1+e^{-w^T\phi(x^{(i)})})^2})(\phi(x^{(i)})^Tx)(\phi(x^{(i)})^Tx)\]
\[=(\frac{e^{-w^T\phi(x^{(i)})}}{(1+e^{-w^T\phi(x^{(i)})})^2})(\phi(x^{(i)})^Tx)^2 \geq 0\]

And so \(Hg_i(w)\) is positive semi-definite \(\implies\) \(g_i(w)\) is
convex. By further results of HW \(7\),
\(\displaystyle \sum_{i=1}^{N}g_i(w)\) is convex too since it is a
finite sum of convex functions.

In addition, since \(\lambda > 0\), the quadratic function of \(w\),
\(\lambda||w||^2=\lambda w^TIw \geq0\) is convex as well.

So the sum, \[\displaystyle \sum_{i=1}^{N}g_i(w)+\lambda||w||^2\] is
convex. This implies,
\[-(\displaystyle \sum_{i=1}^{N}g_i(w)+\lambda||w||^2)\]
\[=-(\displaystyle \sum_{i=1}^{N}((1-y_i)(w^T\phi(x^{(i)}))+\text{log}(1+e^{-w^T\phi(x^{(i)})})+\lambda||w||^2)\]
\[=\displaystyle \sum_{i=1}^{N}((1-y_i)(-w^T\phi(x^{(i)}))-\text{log}(1+e^{-w^T\phi(x^{(i)})})-\lambda||w||^2\]

is concave (by further results of HW \(7\), since we have the negative
of a convex function).

\hypertarget{d-ii.}{%
\section{2. (d) ii.}\label{d-ii.}}

Consider,

\[\max_{w \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-w^T\phi(x^{(i)}))-\text{log}(1+e^{-w^T\phi(x^{(i)})}))-\lambda||w||^2\]
\[=\max_{w \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^Tw)-\text{log}(1+e^{-\phi(x^{(i)})^Tw}))-\lambda||w||^2\]
Let us again write \(w=B^Ta+z\) and make the substitution,
\[\max_{a \in R^N, z \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^T(B^Ta+z))-\text{log}(1+e^{-\phi(x^{(i)})^T(B^Ta+z)}))-\lambda(B^Ta+z)^T(B^Ta+z)\]
\[\max_{a \in R^N, z \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-\phi(x^{(i)})^TB^Ta)-\text{log}(1+e^{-\phi(x^{(i)})^TB^Ta}))-\lambda(a^TBB^T a + a^TB z + z^TB^Ta +z^Tz)\]
\[\max_{a \in R^N, z \in R^m} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^TB\phi(x^{(i)}))-\text{log}(1+e^{-a^TB\phi(x^{(i)})}))-\lambda a^TBB^T a -\lambda z^Tz\]
Now, since we are maximizing and for any \(z\), \(\lambda z^Tz \geq 0\),
we must choose \(z^*=0.\) And so we have,
\[\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^TB\phi(x^{(i)}))-\text{log}(1+e^{-a^TB\phi(x^{(i)})}))-\lambda a^TBB^T a\]
Now, consider \(BB^T\), shown in \(2\).b.ii) to be a \(N \times N\)
kernel matrix, and denote it \(K.\) As also shown in \(2\).b.ii), the
ith column, \(k^{(i)}\), of \(K\) is the ith column of \(BB^T\), which
is \(B\phi(x^{(i)})\). And so, making the respective substitutions we
get,
\[\max_{a \in R^N} \displaystyle \sum_{i=1}^{N}((1-y_i)(-a^Tk^{(i)})-\text{log}(1+e^{-a^Tk^{(i)}}))-\lambda a^TK a\]

\hypertarget{section-2}{%
\section{3.}\label{section-2}}

\hypertarget{a-1}{%
\section{3. (a)}\label{a-1}}

Let \(x, x' \in R^2\) be given. Consider \(\phi(x) \cdot \phi(x'),\)

\[\phi(x) \cdot \phi(x')\]
\[=(1,\sqrt{2}x_1,\sqrt{2}x_2, x_1^2,\sqrt{2}x_1x_2, x_2^2) \cdot (1,\sqrt{2}x'_1,\sqrt{2}x'_2, x_1'^{2},\sqrt{2}x'_1x'_2, x_2'^{2})\]
\[=1+2x_1x'_1+2x_2x'_2+x_1^2x_1'^{2}+2x_1x_2x'_1x'_2+x_2^2x_2'^{2}\]
\[=1+2(x_1x'_1+x_2x'_2)+(x_1^2x_1'^{2}+2x_1x_2x'_1x'_2+x_2^2x_2'^{2})\]
\[=1+2(x_1x'_1+x_2x'_2)+(x_1x'_1+x_2x'_2)^2\]
\[=(1+(x_1x'_1+x_2x'_2))^2\] \[=(1+x \cdot x')^2.\]

\hypertarget{b}{%
\section{3. (b)}\label{b}}

Consider the kernel \(K(x,x')=(1+x \cdot x')^3.\) By the binomial
expansion we have, \[(1+x \cdot x')^3\]
\[=1+3x \cdot x'+ 3(x \cdot x')^2 + (x \cdot x')^3\]
\[=1+3(x_1x'_1+x_2x'_2)+(x \cdot x')^2(3+x \cdot x')\]
\[=1+3(x_1x'_1+x_2x'_2)+(x_1x'_1+x_2x'_2)^2(3+x_1x'_1+x_2x'_2)\]
\[=1+3x_1x'_1+3x_2x'_2+(x_1^2x'^2_1+x_2^2x'^2_2+2x_1x'_1x_2x'_2)(3+x_1x'_1+x_2x'_2)\]
\[=1+3x_1x'_1+3x_2x'_2+3x_1^2x'^2_1+3x_2^2x'^2_2+6x_1x'_1x_2x'_2+x_1^3x'^3_1+x_2^2x'^2_2x_1x'_1+2x^2_1x'^2_1x_2x'_2+x_1^2x'^2_1x_2x'_2+x_2^3x'^3_2+2x_1x'_1x_2^2x'^2_2\]
\[=1+3x_1x'_1+3x_2x'_2+3x_1^2x'^2_1+3x_2^2x'^2_2+6x_1x'_1x_2x'_2+x_1^3x'^3_1+3x_2^2x'^2_2x_1x'_1+3x^2_1x'^2_1x_2x'_2+x_2^3x'^3_2\]
\[=(1,\sqrt{3}x_1, \sqrt{3}x_2, \sqrt{3}x_1^2, \sqrt{3}x_2^2, \sqrt{6}x_1x_2, x_1^3,\sqrt{3}x_1x_2^2,\sqrt{3}x_1^2x_2,x_2^3)\cdot\]
\[(1,\sqrt{3}x'_1, \sqrt{3}x'_2, \sqrt{3}x_1^{'2}, \sqrt{3}x_2^{'2}, \sqrt{6}x'_1x'_2, x_1^{'3},\sqrt{3}x'_1x_2^{'2},\sqrt{3}x_1^{'2}x_2',x_2^{'3})\]
\[=\phi(x)\cdot\phi(x')\] if we define
\(\phi(x)=(1,\sqrt{3}x_1, \sqrt{3}x_2, \sqrt{3}x_1^2, \sqrt{3}x_2^2, \sqrt{6}x_1x_2, x_1^3,\sqrt{3}x_1x_2^2,\sqrt{3}x_1^2x_2,x_2^3).\)

So the feature vector for any \(x \in R^2\) is defined by
\(\phi(x): R^2 \to R^{10}\),
\[\phi(x)=(1,\sqrt{3}x_1, \sqrt{3}x_2, \sqrt{3}x_1^2, \sqrt{3}x_2^2, \sqrt{6}x_1x_2, x_1^3,\sqrt{3}x_1x_2^2,\sqrt{3}x_1^2x_2,x_2^3)\]

\hypertarget{c-and-d}{%
\section{3. (c) and (d)}\label{c-and-d}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{2020}\NormalTok{)}
\NormalTok{data <-}\StringTok{ }\KeywordTok{read.csv}\NormalTok{(}\StringTok{"nn.txt"}\NormalTok{, }\DataTypeTok{header =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{sep =} \StringTok{' '}\NormalTok{, }\DataTypeTok{stringsAsFactors =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{# choosing 500 data points}
\NormalTok{r <-}\StringTok{ }\KeywordTok{sort}\NormalTok{(}\KeywordTok{round}\NormalTok{(}\KeywordTok{runif}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DataTypeTok{min =} \DecValTok{1}\NormalTok{, }\DataTypeTok{max =}\NormalTok{ (}\KeywordTok{nrow}\NormalTok{(data)}\OperatorTok{-}\DecValTok{1}\NormalTok{))))}
\NormalTok{X <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(data[r,}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{y <-}\StringTok{ }\KeywordTok{as.matrix}\NormalTok{(data}\OperatorTok{$}\NormalTok{y[r])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# Let us plot our data}
\KeywordTok{library}\NormalTok{(ggplot2)}
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ X[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{color =} \KeywordTok{as.factor}\NormalTok{(y))) }\OperatorTok{+}
\StringTok{  }\KeywordTok{xlab}\NormalTok{(}\StringTok{"x1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"x2"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{theme}\NormalTok{(}\DataTypeTok{legend.position =} \StringTok{"bottom"}\NormalTok{,}
        \DataTypeTok{legend.background =} \KeywordTok{element_rect}\NormalTok{(}\DataTypeTok{fill =} \StringTok{"#EEEEEE"}\NormalTok{, }\DataTypeTok{color =} \StringTok{"black"}\NormalTok{),}
        \DataTypeTok{legend.title =} \KeywordTok{element_blank}\NormalTok{(),}
        \DataTypeTok{axis.title =} \KeywordTok{element_text}\NormalTok{(}\DataTypeTok{size =} \DecValTok{16}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{Final_files/figure-latex/unnamed-chunk-18-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# construct the kernel matrix}
\NormalTok{power <-}\StringTok{ }\DecValTok{2}
\NormalTok{K <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X))}\OperatorTok{^}\NormalTok{power}

\CommentTok{# log likelihood function}
\NormalTok{log_L <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(a, K, y, lambda)\{}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{sum}\NormalTok{(}\OperatorTok{-}\NormalTok{(}\DecValTok{1}\OperatorTok{-}\NormalTok{y)}\OperatorTok{*}\NormalTok{(K }\OperatorTok{%*%}\StringTok{ }\NormalTok{a) }\OperatorTok{-}\StringTok{ }\KeywordTok{log}\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{a))) }
  \OperatorTok{-}\StringTok{ }\NormalTok{lambda}\OperatorTok{*}\KeywordTok{t}\NormalTok{(a)}\OperatorTok{%*%}\NormalTok{K}\OperatorTok{%*%}\NormalTok{a)}
\NormalTok{\} }

\CommentTok{# gradient of log likelihood}
\NormalTok{grad_logL <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(a, K, y, lambda) \{}
\NormalTok{  total <-}\StringTok{ }\NormalTok{K }\OperatorTok{*}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(y }\OperatorTok{-}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{    }\NormalTok{K }\OperatorTok{*}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(}\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{a)}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{a)))}
\NormalTok{  grad <-}\StringTok{ }\KeywordTok{colSums}\NormalTok{(total) }\OperatorTok{-}\StringTok{ }\NormalTok{lambda}\OperatorTok{*}\DecValTok{2}\OperatorTok{*}\NormalTok{K}\OperatorTok{%*%}\NormalTok{a}
  \KeywordTok{return}\NormalTok{(grad)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

For the hessian calculation let us put everything in vector form. The
hessian is
\[\displaystyle \sum_{i=1}^{N} (-k^{(i)} (k^{(i)})^T[\frac{e^{-a^Tk^{(i)}}}{(1+e^{-a^Tk^{(i)}})^2}])-2\lambda K\]
Which is equivalent to, \[-KDK-2\lambda K\] where \(K\) is the kernel
matrix mentioned above and \(D\) is a diagonal matrix with,
\[d_{ii}=\frac{e^{-a^Tk^{(i)}}}{(1+e^{-a^Tk^{(i)}})^2}\]

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# hessian of log likelihood}
\NormalTok{hessian_logL <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(a, K, lambda)\{}
\NormalTok{  d <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{a)}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{a))}\OperatorTok{^}\DecValTok{2}
\NormalTok{  D <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\KeywordTok{c}\NormalTok{(d))}
\NormalTok{  hessian <-}\StringTok{ }\OperatorTok{-}\NormalTok{K }\OperatorTok{%*%}\StringTok{ }\NormalTok{D }\OperatorTok{%*%}\StringTok{ }\NormalTok{K }\OperatorTok{-}\StringTok{ }\NormalTok{lambda}\OperatorTok{*}\DecValTok{2}\OperatorTok{*}\NormalTok{K}
  \KeywordTok{return}\NormalTok{(hessian)}
\NormalTok{\}}

\CommentTok{###########################################}
\CommentTok{# let's used Damped Newton's Method       #}
\CommentTok{# Damped Newton's Method will converge    #}
\CommentTok{# since the log likelihood is concave and # }
\CommentTok{# so will pick ascent directions          #}
\CommentTok{###########################################}

\NormalTok{norm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x) }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{sum}\NormalTok{(x}\OperatorTok{^}\DecValTok{2}\NormalTok{))}

\NormalTok{damped_NM <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(a, K, y, lambda, }\DataTypeTok{eps =} \FloatTok{1e-10}\NormalTok{, }\DataTypeTok{print =}\NormalTok{ F, }\DataTypeTok{graph =}\NormalTok{ F) \{}

\NormalTok{  g <-}\StringTok{ }\KeywordTok{grad_logL}\NormalTok{(a, K, y, lambda)}
\NormalTok{  i <-}\StringTok{ }\DecValTok{1}
\NormalTok{  s <-}\StringTok{ }\DecValTok{1}
\NormalTok{  I <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(}\KeywordTok{length}\NormalTok{(g))}
\NormalTok{  x_values <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
\NormalTok{  y_values <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
  
  \ControlFlowTok{while}\NormalTok{(}\KeywordTok{norm}\NormalTok{(g) }\OperatorTok{>}\StringTok{ }\NormalTok{eps) \{}
\NormalTok{    h <-}\StringTok{ }\KeywordTok{hessian_logL}\NormalTok{(a, K, lambda)}
    
    \CommentTok{# let's make sure we can invert the hessian}
    \CommentTok{# otherwise, we need modification}
\NormalTok{    alpha <-}\StringTok{ }\DecValTok{0}
    \ControlFlowTok{while}\NormalTok{ (}\KeywordTok{kappa}\NormalTok{(h }\OperatorTok{+}\StringTok{ }\NormalTok{alpha}\OperatorTok{*}\NormalTok{I) }\OperatorTok{>}\StringTok{ }\FloatTok{1e15}\NormalTok{)\{}
\NormalTok{      alpha <-}\StringTok{ }\DecValTok{2}\OperatorTok{*}\NormalTok{(alpha }\OperatorTok{+}\StringTok{ }\FloatTok{0.01}\NormalTok{)}
\NormalTok{    \}}
    
\NormalTok{    h <-}\StringTok{ }\NormalTok{h }\OperatorTok{+}\StringTok{ }\NormalTok{alpha}\OperatorTok{*}\NormalTok{I}
\NormalTok{    d <-}\StringTok{ }\OperatorTok{-}\KeywordTok{solve}\NormalTok{(h,g)}
    
    \CommentTok{# printing option}
\NormalTok{    l <-}\StringTok{ }\KeywordTok{log_L}\NormalTok{(a, K, y, lambda)}
    \ControlFlowTok{if}\NormalTok{ (print)\{}
      \KeywordTok{cat}\NormalTok{(}\StringTok{"iteration"}\NormalTok{, i, }\StringTok{"logL ="}\NormalTok{, l, }\StringTok{"}\CharTok{\textbackslash{}n}\StringTok{"}\NormalTok{)}
\NormalTok{    \}}
    
    \CommentTok{# graphing option}
    \ControlFlowTok{if}\NormalTok{ (graph)\{}
\NormalTok{      x_values <-}\StringTok{ }\KeywordTok{append}\NormalTok{(x_values, i)}
\NormalTok{      y_values <-}\StringTok{ }\KeywordTok{append}\NormalTok{(y_values, l)}
\NormalTok{    \}}
    
    \CommentTok{# backtracking}
    \ControlFlowTok{while}\NormalTok{ (l }\OperatorTok{>}\StringTok{ }\KeywordTok{log_L}\NormalTok{(a }\OperatorTok{+}\StringTok{ }\NormalTok{s}\OperatorTok{*}\NormalTok{d, K, y, lambda))\{}
\NormalTok{      s <-}\StringTok{ }\NormalTok{s}\OperatorTok{/}\DecValTok{2}
\NormalTok{    \}}

    \CommentTok{# update parameters}
\NormalTok{    a <-}\StringTok{ }\NormalTok{a }\OperatorTok{+}\StringTok{ }\NormalTok{s}\OperatorTok{*}\NormalTok{d}
    
    \ControlFlowTok{if}\NormalTok{(}\KeywordTok{abs}\NormalTok{(l}\OperatorTok{-}\KeywordTok{log_L}\NormalTok{(a }\OperatorTok{+}\StringTok{ }\NormalTok{s}\OperatorTok{*}\NormalTok{d, K, y, lambda)) }\OperatorTok{<}\StringTok{ }\NormalTok{eps)\{}
      \ControlFlowTok{break}
\NormalTok{    \}}
    
\NormalTok{    g <-}\StringTok{ }\KeywordTok{grad_logL}\NormalTok{(a, K, y, lambda)}
\NormalTok{    i <-}\StringTok{ }\NormalTok{i }\OperatorTok{+}\StringTok{ }\DecValTok{1}
\NormalTok{\}}
  
  \KeywordTok{return}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{max =}\NormalTok{ a, }\DataTypeTok{iter =}\NormalTok{ i, }\DataTypeTok{x_values =}\NormalTok{ x_values, }\DataTypeTok{y_values =}\NormalTok{ y_values, }\DataTypeTok{grad =}\NormalTok{ g))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# defining classifier}
\NormalTok{classifier <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(X_test, a_star, p)\{}
\NormalTok{  t <-}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{+}\StringTok{ }\NormalTok{X }\OperatorTok{%*%}\StringTok{ }\KeywordTok{t}\NormalTok{(X_test))}\OperatorTok{^}\NormalTok{power}
\NormalTok{  prob <-}\StringTok{ }\DecValTok{1}\OperatorTok{/}\NormalTok{(}\DecValTok{1}\OperatorTok{+}\KeywordTok{exp}\NormalTok{(}\OperatorTok{-}\KeywordTok{t}\NormalTok{(t) }\OperatorTok{%*%}\StringTok{ }\NormalTok{a_star))}
\NormalTok{  y_pred <-}\StringTok{ }\KeywordTok{as.numeric}\NormalTok{(prob }\OperatorTok{>=}\StringTok{ }\NormalTok{p)}
  \KeywordTok{return}\NormalTok{(}\KeywordTok{list}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ y_pred, }\DataTypeTok{p =}\NormalTok{ prob))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{# plotting function, given lambda and a probability cut-off}
\NormalTok{plotting <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(lambda, p)\{}
  \CommentTok{# initializing a}
\NormalTok{  a <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{500}\NormalTok{)}
\NormalTok{  damped_NM_soln <-}\StringTok{ }\KeywordTok{damped_NM}\NormalTok{(a, K, y, l)}
\NormalTok{  a_star <-}\StringTok{ }\NormalTok{damped_NM_soln}\OperatorTok{$}\NormalTok{max}
\NormalTok{  pred_y <-}\StringTok{ }\KeywordTok{classifier}\NormalTok{(X_test, a_star, p)}\OperatorTok{$}\NormalTok{y}
  \KeywordTok{print}\NormalTok{(}\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{    }\KeywordTok{geom_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ X_test[,}\DecValTok{1}\NormalTok{], }\DataTypeTok{y =}\NormalTok{ X_test[,}\DecValTok{2}\NormalTok{], }\DataTypeTok{color =} \KeywordTok{as.factor}\NormalTok{(pred_y)), }\DataTypeTok{show.legend =}\NormalTok{ F) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \KeywordTok{paste}\NormalTok{(}\KeywordTok{paste}\NormalTok{(}\StringTok{"p = "}\NormalTok{, p), }\KeywordTok{paste}\NormalTok{(}\StringTok{" || lambda = "}\NormalTok{, lambda))) }\OperatorTok{+}
\StringTok{    }\KeywordTok{xlab}\NormalTok{(}\StringTok{"x1"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylab}\NormalTok{(}\StringTok{"x2"}\NormalTok{))}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{set.seed}\NormalTok{(}\DecValTok{1}\NormalTok{)}
\CommentTok{# choose test points}
\NormalTok{X_test =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(}\DecValTok{4}\OperatorTok{*}\KeywordTok{runif}\NormalTok{(}\DecValTok{10000}\NormalTok{)}\OperatorTok{-}\DecValTok{2}\NormalTok{, }\DecValTok{4}\OperatorTok{*}\KeywordTok{runif}\NormalTok{(}\DecValTok{10000}\NormalTok{)}\OperatorTok{-}\DecValTok{2}\NormalTok{)}
\CommentTok{# let us fit for these given values of lambda}
\NormalTok{lambda <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{50}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{500}\NormalTok{, }\DecValTok{1000}\NormalTok{, }\DecValTok{10000}\NormalTok{)}
\CommentTok{# let us first choose a probability cut-off of 50%}
\ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in}\NormalTok{ lambda)\{}
  \KeywordTok{plotting}\NormalTok{(l, }\FloatTok{0.5}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-1} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-2} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-3} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-4} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-5} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-6} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-7} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-8} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-23-9} \end{center}

Now let us fit for the same values of lambda but try for a probability
cut-off of 30\% and 70\%.

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in}\NormalTok{ lambda)\{}
  \KeywordTok{plotting}\NormalTok{(l, }\FloatTok{0.3}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-1} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-2} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-3} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-4} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-5} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-6} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-7} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-8} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-24-9} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{for}\NormalTok{(l }\ControlFlowTok{in}\NormalTok{ lambda)\{}
  \KeywordTok{plotting}\NormalTok{(l, }\FloatTok{0.7}\NormalTok{)}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-1} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-2} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-3} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-4} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-5} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-6} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-7} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-8} \includegraphics[width=.49\linewidth]{Final_files/figure-latex/unnamed-chunk-25-9} \end{center}

It looks like we get a better fit when lambda is very small (approx
\(0\)). This makes sense given \# of parameters vs. \# of data points so
you should be able to see decent results without needing to add a
penalty term to the regression (i.e., \(\lambda = 0\)). The shape of the
region for which the classifier predicts \(1\) is an ellipse/circle
(essentially, has a round shape). This also makes sense since we are
using a quadratic polynomial kernel (the boundary should be a curve
based on the degree of the polynomial, in this case \(2\)).

As an aside: We could do something else, we have a dataset with close to
\(2000\) points and we only trained on/used \(500\). We can take the
remaning \textasciitilde{} \(1500\) points and treat it as our test
dataset, and so instead of testing with visualizations we can look at
accuracy (since our test dataset, \textasciitilde{} \(1500\) points, are
labeled).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(}\StringTok{"reshape2"}\NormalTok{)}
\CommentTok{# let's look at overall accuracy for different combinations}
\CommentTok{# of lambda values and probability cut-offs}
\CommentTok{# let's use the remaining points to test}
\NormalTok{X_test <-}\StringTok{ }\NormalTok{data[}\KeywordTok{setdiff}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),r),}\DecValTok{1}\OperatorTok{:}\DecValTok{2}\NormalTok{]}
\NormalTok{y_test <-}\StringTok{ }\NormalTok{data[}\KeywordTok{setdiff}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\KeywordTok{nrow}\NormalTok{(data),r),}\DecValTok{3}\NormalTok{]}

\NormalTok{accuracy <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(y_test, pred_y) \{}
\NormalTok{  sens <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(pred_y}\OperatorTok{==}\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{y_test}\OperatorTok{==}\DecValTok{1}\NormalTok{)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(y_test}\OperatorTok{==}\DecValTok{1}\NormalTok{)}
\NormalTok{  spec <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(pred_y}\OperatorTok{==}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{y_test}\OperatorTok{==}\DecValTok{0}\NormalTok{)}\OperatorTok{/}\KeywordTok{sum}\NormalTok{(y_test}\OperatorTok{==}\DecValTok{0}\NormalTok{)}
\NormalTok{  overall <-}\StringTok{ }\NormalTok{(}\KeywordTok{sum}\NormalTok{(pred_y}\OperatorTok{==}\DecValTok{1} \OperatorTok{&}\StringTok{ }\NormalTok{y_test}\OperatorTok{==}\DecValTok{1}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{sum}\NormalTok{(pred_y}\OperatorTok{==}\DecValTok{0} \OperatorTok{&}\StringTok{ }\NormalTok{y_test}\OperatorTok{==}\DecValTok{0}\NormalTok{))}\OperatorTok{/}\KeywordTok{length}\NormalTok{(y_test)}
  \KeywordTok{return}\NormalTok{ (}\KeywordTok{list}\NormalTok{(}\DataTypeTok{sensitivity=}\NormalTok{sens, }\DataTypeTok{specificity=}\NormalTok{spec, }\DataTypeTok{overall=}\NormalTok{overall))}
\NormalTok{\}}

\NormalTok{probs <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{0.2}\NormalTok{, }\FloatTok{0.3}\NormalTok{, }\FloatTok{0.4}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.6}\NormalTok{, }\FloatTok{0.7}\NormalTok{, }\FloatTok{0.8}\NormalTok{, }\FloatTok{0.9}\NormalTok{)}
\NormalTok{lambda <-}\StringTok{ }\KeywordTok{seq}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{100}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\NormalTok{plotting}\FloatTok{.2}\NormalTok{ <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(x, lambda)\{}
\NormalTok{  a <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{500}\NormalTok{)}
\NormalTok{  acc <-}\StringTok{ }\KeywordTok{c}\NormalTok{()}
  \ControlFlowTok{for}\NormalTok{ (l }\ControlFlowTok{in}\NormalTok{ lambda)\{}
\NormalTok{    a <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{500}\NormalTok{)}
\NormalTok{    damped_NM_soln <-}\StringTok{ }\KeywordTok{damped_NM}\NormalTok{(a, K, y, l)}
\NormalTok{    a_star <-}\StringTok{ }\NormalTok{damped_NM_soln}\OperatorTok{$}\NormalTok{max}
\NormalTok{    pred_y <-}\StringTok{ }\KeywordTok{classifier}\NormalTok{(X_test, a_star, x)}\OperatorTok{$}\NormalTok{y}
\NormalTok{    acc <-}\StringTok{ }\KeywordTok{c}\NormalTok{(acc, }\KeywordTok{accuracy}\NormalTok{(y_test, pred_y)}\OperatorTok{$}\NormalTok{overall}\OperatorTok{*}\DecValTok{100}\NormalTok{)}
\NormalTok{  \}}
  \KeywordTok{return}\NormalTok{(acc)}
\NormalTok{\}}

\NormalTok{accuracies <-}\StringTok{ }\KeywordTok{sapply}\NormalTok{(probs, }\DataTypeTok{FUN=}\NormalTok{plotting}\FloatTok{.2}\NormalTok{, }\DataTypeTok{lambda =}\NormalTok{ lambda)}
\NormalTok{df <-}\StringTok{ }\KeywordTok{data.frame}\NormalTok{(accuracies, }\DataTypeTok{Lambda=}\DecValTok{1}\OperatorTok{:}\DecValTok{101}\NormalTok{)}
\NormalTok{df <-}\StringTok{ }\KeywordTok{melt}\NormalTok{(df, }\DataTypeTok{id.vars =} \StringTok{'Lambda'}\NormalTok{, }\DataTypeTok{variable.name =} \StringTok{'series'}\NormalTok{)}

\KeywordTok{ggplot}\NormalTok{(df, }\KeywordTok{aes}\NormalTok{(Lambda, value, }\DataTypeTok{color =}\NormalTok{ series)) }\OperatorTok{+}\StringTok{ }\KeywordTok{geom_line}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{ylab}\NormalTok{(}\StringTok{"Accuracy (%)"}\NormalTok{) }\OperatorTok{+}\StringTok{ }\KeywordTok{ylim}\NormalTok{(}\KeywordTok{c}\NormalTok{(}\DecValTok{40}\NormalTok{,}\DecValTok{100}\NormalTok{)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale_color_manual}\NormalTok{(}\StringTok{"Prob Cutt-off"}\NormalTok{, }\DataTypeTok{values =} \KeywordTok{c}\NormalTok{(}\StringTok{"#008000"}\NormalTok{, }\StringTok{"#000080"}\NormalTok{, }\StringTok{"#FF6347"}\NormalTok{, }
            \StringTok{"#FFD700"}\NormalTok{, }\StringTok{"#00CED1"}\NormalTok{, }\StringTok{"#8A2BE2"}\NormalTok{, }\StringTok{"#BC8F8F"}\NormalTok{, }\StringTok{"#FF00FF"}\NormalTok{), }
            \DataTypeTok{labels =} \KeywordTok{c}\NormalTok{(}\StringTok{"20%"}\NormalTok{, }\StringTok{"30%"}\NormalTok{, }\StringTok{"40%"}\NormalTok{, }\StringTok{"50%"}\NormalTok{, }\StringTok{"60%"}\NormalTok{, }\StringTok{"70%"}\NormalTok{, }\StringTok{"80%"}\NormalTok{, }\StringTok{"90%"}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\includegraphics{plots.png}

We can see from the above plot that we would get the highest accuracy on
the test dataset (about \(93\)\%) if we choose \(\lambda=0\) and a
probability cut-off of \(50\)\%.

\hypertarget{e}{%
\section{3. (e)}\label{e}}

It is reasonable to suppose that the cubic polynomial kernel would be
more accurate than the quadratic polynomial kernel. With the cubic
polynomial kernel we are capturing more features, and the boundary
should be a curve based on a cubic polynomial and so, is less
restrictive than the shapes allowed by a quadratic polynomial kernel
(and so we might be able to capture some part of the antenna).

\end{document}
