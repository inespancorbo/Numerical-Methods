# Brief Summary

PART 1
- Linear Regression as an Optimization
- Optimization (convex, non-convex)
- Optimization of a quadratic function
- Optimization of Sum of Squared Residual Loss Function/Linear Regression

PART 2
- Logistic regression as a non-quadratic optimization
- the general optimization iteration
- gradient algorithms: steepest ascent/descent (backtracking, stochastic, mini-batch)
- root finding: bisection and Newton's method

PART 3
- Multi-dimensional Newton's method
- Gaussian elimination
- Time and space complexity 

PART 4
- Newton's method for optimization as root finding
- Newton's method for optimization as quadratic optimization
- Spectral Decomposition Thm
- Application of spectral decomposition to quadratics min/max/saddle

PART 5
- why steepest descent zig-zags
- why Newton's method chooses good directions
- convex functions
- computer arithmetic, error

PART 6
- Page rank algorithm
- power iteration algorithm
- positive definite matrices
- descent algorithms and descent directions
- Newton's method as a descent algorithm for convex functions

PART 7
- Condition number of matrices 
- Condition numbers and round off error
- damped Newton's method and convex functions

PART 8
- damped Newton's method with hessian modification
- PCA and SVD

PART 9
- QR decomposition
- Gramm-Schmidt orthogonalization

PART 10
- Interpolation
- Orthogonalized power iteration

PART 11
- Spline regression
- Numerical differentiation and integration

PART 12
- Neural Networks
- Kernel Machines
