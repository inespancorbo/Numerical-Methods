# Brief Summary of Material Covered

PART 1, Part 1
- Linear Regression as an Optimization
- Optimization (convex, non-convex)

PART 1, Part 2
- Optimization of a quadratic function
- Optimization of Sum of Squared Residual Loss Function/Linear Regression

PART 2, Part 1
- Course description
- Logistic regression as a non-quadratic optimization

PART 2, Part 2
- the general optimization iteration
- steepest ascent, backtracking
- root finding:  bisection and Newton's method (see ROOT FINDING mp4 file)

PART 3, Part 1
- Multi-dimensional Newton's method

PART 3, Part 2
- Gaussian elimination
- Time and Space complexity 

PART 4, Part 1
- Newton's method for optimization as root finding
- Newton's method for optimization as quadratic optimization

PART 4, Part 2
- Spectral Decomposition Thm
- Application of spectral decomposition to quadratics min/max/saddle

PART 5, Part 1
- why steepest descent zig-zags
- why Newton's method chooses good directions
- convex functions

PART 5, Part 2
- computer arithmetic

PART 6, Part 1
- Page rank algorithm
- power iteration algorithm

PART 6, Part 2
- postive definite matrices
- descent algorithms and descent directions
- Newton's method as a descent algorithm for convex functions

PART 7, Part 1
- Condition number of matrices 
- Condition numbers and round off error

PART 7, Part 2
- damped Newton's method and convex functions

PART 8, Part 1
- damped Newton's method with hessian modification

PART 8, Part 2
- PCA

PART 9, Part 1
- PCA continued

PART 9, Part 2
- QR decomposition
- Gramm-Schmidt orthogonalization

PART 10
- Interpolation
- Orthogonalized power iteration

PART 11
- Spline regression
- Numerical differentiation and integration

PART 12
- Neural Networks
- Kernel Machines
