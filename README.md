# Brief Summary of Material Covered

PART 1, Part 1
1. Linear Regression as an Optimization
2. Optimization (convex, non-convex)

PART 1, Part 2
4. Optimization of a quadratic function
5. Optimization of Sum of Squared Residual Loss Function/Linear Regression

PART 2, Part 1
1. Course description
2. Logistic regression as a non-quadratic optimization

PART 2, Part 2
3. the general optimization iteration
4. steepest ascent, backtracking
5. root finding:  bisection and Newton's method (see ROOT FINDING mp4 file)

PART 3, Part 1
1. multi-dimensional Newton's method

PART 3, Part 2
2. Gaussian elimination
3. Time and Space complexity 

PART 4, Part 1
1. Newton's method for optimization as root finding
2. Newton's method for optimization as quadratic optimization

PART 4, Part 2
1. Spectral Decomposition Thm
2. application of spectral decomposition to quadratics min/max/saddle

PART 5, Part 1
1. why steepest descent zig-zags
2. why Newton's method chooses good directions
3. convex functions

PART 5, Part 2
4. computer arithmetic

PART 6, Part 1
1. Page rank algorithm
2. power iteration algorithm

PART 6, Part 2
3. postive definite matrices
4. descent algorithms and descent directions
5. Newton's method as a descent algorithm for convex functions

PART 7, Part 1
1. Condition number of matrices 
2. Condition numbers and round off error

PART 7, Part 2
3. damped Newton's method and convex functions

PART 8, Part 1
1. damped Newton's method with hessian modification

PART 8, Part 2
2. PCA

PART 9, Part 1
1. PCA continued

PART 9, Part 2
2. QR decomposition
3. Gramm-Schmidt orthogonalization

PART 10
1. Interpolation
2. Orthogonalized power iteration

PART 11
1. Spline regression
2. Numerical differentiation and integration

PART 12
1. Neural Networks
2. Kernel Machines
